\chapter{A simple molecular dynamics program?}
Intro

\section{The main program}
% Deducing how a molecular dynamics program does its simulations isn't always easy from looking at source code, but most programs will follow a flow similar to the following:
%
Most molecular dynamics programs will follow an flow similar to the following:
%
\begin{itemize}[midsep]
    \renewcommand{\labelitemii}{$\bullet$} % Set list depth 2 bullet thing equal to first
    \item Initialize the system. Set up the initial positions and velocities for all atoms, either by generating them or loading a saved state from a previous simulation.\todo{remove drift!}
    \item For each timestep
    \begin{itemize}[midsep]
        \item Calculate the forces between the atoms.
        \item Integrate Newton's equations of motion using an appropriate integration scheme \todo{examples?}.
        \item Sample the values of the quantities we want to study, and add to the averages.
    \end{itemize}
    \item After all timesteps have been finished we print out the measured quantities, and we could also save the state of the system so we can continue from this state later.
\end{itemize}
%
An example of a program that implements the above procedure can be seen in \cref{list:simple_md_program}.
%
\begin{listing}[!htb]%
\begin{cppcode*}{gobble=4}
    System system = initializeSystem(parameters);
    double time = 0.0;
    double dt = 0.01;
    for (double time = 0; time < tMax; time += dt)
    {
        calculateForces(system);
        integrateEquationsOfMotion(system, dt);
        sample(system);
    }
\end{cppcode*}
\caption{%
    An example of a typical implementation of a molecular dynamics program using object-oriented programming. See \cref{list:calculate_forces,list:regular_verlet,list:sampling} for examples of implementations of the functions \texttt{calculateForces}, \texttt{integrateEquationsOfMotion}, and \texttt{sample}.%
    \label{list:simple_md_program}%
}%
\end{listing}%

When starting a new simulation we usually initialize the positions of the atoms by putting them on a regular grid like a face-centered cubic (fcc), a body-centered cubic (bcc), or a simple cubic grid. The purpose of this is to not have any atoms too close to each other, which we see from the $r^{-12}$ term would give very big forces\todo{not defined yet... move this section?}, and to start with the atoms in a state from which we are able to quickly get to the state we want to study. If we for example want to study a liquid argon system, it is wise to start in an unstable crystal state, by for example using a low density or high temperature, so that the system would melt spontaneously when we start the simulation.

\section{Calculation of forces}
The forces are calculated from the derivatives of interatomic potentials, that depend on the positions of the atoms, and generally are of the form
\begin{align*}
    U(\rvec) = \sum_{i<j} U_{ij}(r_{ij}) + \sum_{i<j<k} U_{ijk}(\rvec_i, \rvec_i, \rvec_k) + \dots,
\end{align*}
where $\rvec_i$ is the position of atom $i$, $r_{ij}$ is the distance between atom $i$ and $j$, $U_{ij}$ is a two-particle potential depending only on the distance between two atoms, and $U_{ijk}$ is a three-particle potential that usually also depends on the angle between three atoms. Higher-order contributions are also often used\todo{ReaxFF}, but they are very demanding to evaluate numerically.

When developing potentials from quantum mechanical calculations one has to weigh the benefits of having a complex potential that models the interactions accurately, against having a less comples potential that will be easier to implement, and faster to evaluate. The limiting factor in any molecular dynamics calculation is the cost of doing simulations on high-performance computing clusters\hl{like abel}, but luckily it seems like the progress in \hl{CPU} development still seems to follow Moore's Law\hl{source?}, that states that the number of transistors on integrated circuits double approximately every two years\cite{moore1965cramming}, effectively halving the price of doing a computation\hl{source?}.

In this example we will be using a potential first seen as early as 1924\cite{jones1924potential} called the Lennard-Jones potential, \hl{after its creator}. The potential is a simple two-particle potential with the following form
% To model a simple mono-atomic system we use the well-known Lennard-Jones potential\cite{jones1924potential}, which when applied on the noble gas \hl{(inert)} Argon gives results that are in good agreement with experimental results. The potential is usually written as follows
\begin{align}
%     U(r) = 4\varepsilon \Big[
%     \underbrace{
%         \left(\frac{\sigma}{r}\right)^{12}
%     }_{\text{attraction}}
%      - 
%     \underbrace{
%         \left(\frac{\sigma}{r}\right)^6
%     }_{\text{repulsion}}
%     \Big],
    U(r_{ij}) = 4\varepsilon\left[ \left(\frac{\sigma}{r}\right)^{12} - \left(\frac{\sigma}{r}\right)^{6} \right]
    = \varepsilon\left[ \left(\frac{r_m}{r}\right)^{12} - 2\left(\frac{r_m}{r}\right)^{6} \right],
    \label{eq:lennard-jones_potential}
\end{align}
where $\sigma$ is the distance between where the potential is zero \hl{(the equilibrium distance between the atoms)}, $\varepsilon$ is related to the strength of the potential (the minimum value of the potential), and $r_m = 2^{1/6} \sigma$ is the distance where the potential is at its minimum. The $r^{-12}$ term is a repulsive term that describes \hl{Pauli repulsion/steric repulsion/overlap of electron orbitals} and the $r^{6}$ term is an attractive term that describes \hl{long range/van der Waals/dipole-dipole/dispersion} interactions. \todo{something about physical justification, use 6 for repulsive because $r^{12} = (r^6)^2$}. 

The potential was used by Lennard-Jones to study the \hl{noble gas} Argon, and has been used by many others\todo{to study what? Examples}. See \cref{fig:lennard-jones_potential} for a plot of the potential using the parameters usually used for simulating Argon\cite{frenkel2001understanding}, $\sigma = 3.405$~\AA\ and $\varepsilon = 0.010318$~eV.
%
\begin{figure}[htpb]%
    \centering%
    \includesvg[width=0.7\textwidth, svgpath=./images/lennard-jones/]{lennard-jones_manualalign}%
%     \includesvg[width=0.7\textwidth, svgpath=./images/lennard-jones/]{lennard-jones}%
    \caption{%
        Plot of the Lennard-Jones potential, as stated in \cref{eq:lennard-jones_potential}. Using the parameters usually used for simulating Argon\cite{frenkel2001understanding}, $\sigma = 3.405$~\AA\ and $\varepsilon = 0.010318$~eV.%
        \label{fig:lennard-jones_potential}%
    }%
\end{figure}%

\subsection{Newton's third law}
When calculating two-particle forces like the Lennard-Jones potential there is a simple optimization that lets us halve the number of computations, by utilizing Newton's third law. We see that when evaluating $U(r_{ij})$, the force will have the same magnitude if we switch particle $i$ and $j$. This means that when we have calculated the force $\vec F_{ij}$, from particle $j$ on particle $i$, we know that the force on atom $j$ from particle $i$ will have the same magnitude, and we can simply add the opposite force to atom $j$, $\vec F_{ji} = -\vec F_{ij}$. This way we only have to calculate the forces between particle $i$ and particles $j>i$ in the main force loop.

See \cref{list:calculate_forces,list:calculate_force_between_atoms} for an example of how to implement force calculation using the Lennard-Jones potential, using the optimization from Newton's third law.
%
\begin{listing}[!htb]%
% \begin{cppcode*}{gobble=4}
%     void calculateForces(System &system)
%     {
%         for (Atom *atom1 : system.atoms())
%         {
%             for (Atom *atom2 : system.atoms())
%             {
%                 
%             }
%         }
%     }
% \end{cppcode*}
%         for (vector<Atom*>::iterator atom1 = atoms.begin(); atom1 != atoms.end(); ++atom1)
\begin{cppcode*}{gobble=4}
    void calculateForces(System &system)
    {
        const vector<Atom*> &atoms = system.atoms();
        for (auto atom1 = atoms.begin(); atom1 != atoms.end(); ++atom1)
        {
            // Use Newton's third law to skip half the force calculations
            for (auto atom2 = atom1.next(); atom2 != atoms.end(); ++atom2)
            {
                vec3 force = calculateForceBetweenTwoAtoms(*atom1, *atom2);
                
                (*atom1)->force() += force;
                (*atom2)->force() -= force; // Newton's third law
            }
        }
    }
\end{cppcode*}
\caption{%
%     An example of how to implement the velocity Verlet integration scheme using \cpp-like object-oriented programming.%
    Implementation of \texttt{calculateForces} from \cref{list:simple_md_program}.%
    \label{list:calculate_forces}%
}%
\end{listing}%
%
\begin{listing}[!htb]%
\begin{cppcode*}{gobble=4}
    vec3 calculateForceBetweenTwoAtoms(Atom *atom1, Atom *atom2)
    {
        vec3 drVec = atom1->position() - atom2->position();
        
        double dr2 = drVec.lengthSquared();
        double dr6 = dr2*dr2*dr2;

        double LJforce = 24.0*(2.0 - dr6)/(dr6*dr6*dr2);
        vec3 force = drVec*LJforce;
        
        return force;
    }
\end{cppcode*}
\caption{%
%     An example of how to implement the velocity Verlet integration scheme using \cpp-like object-oriented programming.%
    Implementation of \texttt{calculateForceBetweenTwoAtoms} from \cref{list:calculate_forces}.%
    \label{list:calculate_force_between_atoms}%
}%
\end{listing}%

\section{Integration scheme}
To integrate the Newton's equations of motion for the intermolecular potential there are a lot of different methods to choose between, ranging from the simple forward Euler method\hl{cite} first described by Leonard Euler in 1768, to higher order predictor-corrector methods\hl{cite}. 

It turns out that a deceptively simple method first described by Loup Verlet in 1967\cite{verlet1967computer} often satisfies our needs in an integrator, being both very accurate over long simulation times, having a \hl{global/accumulated} error of the order $\mathcal{O}(\Delta t^2)$\todo{either \cite{thijssen1999computational} sec. 8.4.1-8.4.3 or \cite{frenkel2001understanding} sec. 4.3.3, or derive self in appendix}., and numerically cheap \hl{(compared to other methods)} \hl{requiring on the order of $N$?? flops}. The Verlet method has many variations, but the simplest form \hl{(the one used/described by Verlet)} has the form
\begin{align}
    \rvec(t + \Delta t) \approx 2\rvec(t) - \rvec(t - \Delta t) + \avec(t)\Delta t^2,
    \label{eq:regular_verlet}
\end{align}
where $\Delta t$ is the timestep\todo{define timestep}, and $\avec(t)$ is the velocity at time $t$. This form of the scheme has a truncation error in the position for one timestep of the order $\mathcal{O}(\Delta t^4)$ \hl{show this}.

The stability and \hl{versatiliy} of the \hl{velocity} Verlet method comes from the fact that the scheme is symplectic\todo{show this?, appendix material}. \hl{write more about this, what this means}.

We see that the velocity isn't explicitly calculated or used in this form of the sceme, but if we need it for our experiments we can estimate the velocity using a Taylor expansion around $\rvec(t\pm\Delta t)$, which gives
\begin{align*}
    \vvec(t) = \frac{\rvec(t + \Delta t) - \rvec(t - \Delta t)}{2\Delta t},
\end{align*}
which has a truncation error for one timestep of the order $\mathcal{O}(\Delta t^2)$ \hl{show this}. 

The implementation of the Verlet scheme is mostly straightforward, the only thing we have to take care of happens in the first step. When calculating the positions in the first step, $\rvec(0+\Delta t)$, we see from \cref{eq:regular_verlet} that we need the positions from the previous step, $\rvec(0-\Delta t)$. These positions are usually \hl{simply} approximated using the initial velocity, as follows\todo{is this bad?}
\begin{align*}
    \rvec(0-\Delta t) = \rvec(0) - \vvec(0)\Delta t.
\end{align*}
See \cref{list:regular_verlet} for an example of how to implement the Verlet integration scheme.
%
\begin{listing}[!htb]%
\begin{cppcode*}{gobble=4}
    void integrateEquationsOfMotion(System &system, double dt)
    {
        for (Atom *atom : system.atoms())
        {
            vec3 newPosition = 2.0*atom->position() - atom->oldPosition() 
                               + atom->force()*dt*dt;
            atom->oldPosition() = atom->position();
            atom->position() = newPosition;
            atom->velocity() = (atom->position() - atom->oldPosition())
                               /(2.0*dt);
        }
    }
\end{cppcode*}
\caption{%
%     An example of how to implement the velocity Verlet integration scheme using \cpp-like object-oriented programming.%
    Implentation of \texttt{integrateEquationsOfMotion} from \cref{list:simple_md_program}.%
    \label{list:regular_verlet}%
}%
\end{listing}%

The most used form of the Velocity integration scheme is called the velocity Verlet method\cite{swope1982computer}, and it has the form
\begin{align}
    \rvec(t + \Delta t) &= \rvec(t) + \vvec(t)\Delta t + \avec(t)\frac{\Delta t^2}{2}, \label{eq:velocity_verlet_position}\\
    \vvec(t + \Delta t) &= \vvec(t) + \big[\avec(t) + \avec(t + \Delta t)\big] \frac{\Delta t}{2}, \label{eq:velocity_verlet_velocity}
\end{align}
with the truncation error for one timestep $\Delta t$ being of the order $\mathcal{O}(\Delta t^3)$ for both the position and the velocity, and the \hl{global/accumulated} error being of the order $\mathcal{O}(\Delta t^2)$\hl{cite/show equivalent to regular Verlet}. 

One advantage of this form is that it is self-starting. In the regular Verlet algorithm we need $\rvec(t-\Delta t)$ to compute $\rvec(t+\Delta t)$, which we don't have at $t = 0$. This means that we have to approximate $\rvec(-\Delta T)$ somehow. In the velocity form of the algorithm we only need the positions, velocities and forces at time $t$ to calculate $\rvec(t+\Delta)$.

\hl{Show that it's equivalent to regular Verlet? to rationalize that the accumulated error is the same?}

The velocity Verlet algorithm is usually rewritten in the following way, \hl{to optimize the implementation on a computer}. We see that the new velocities can be written as
\begin{align}
    \vvec(t+\Delta t) = \tilde\vvec(t + \tfrac{1}{2}\Delta t) + \avec(t+\Delta t)\frac{\Delta t}{2}, \label{eq:verlet_velocity_with_halfstep}
\end{align}
where
\begin{align}
    \tilde\vvec(t + \tfrac{1}{2}\Delta t) = \vvec(t) + \avec(t)\frac{\Delta t}{2}.\label{eq:verlet_halfstep}
\end{align}
We see that \cref{eq:verlet_halfstep} can be used in updating the positions, so we rewrite \cref{eq:velocity_verlet_position} to
\begin{align}
    \rvec(t + \Delta t) &= \rvec(t) + \tilde\vvec(t+\tfrac{1}{2}\Delta t)\Delta t.\label{eq:velocity_verlet_positions_halfstep}
\end{align}
Which leads us to the usual way of implementing the algorithm\cite{allen1989computer}:
\begin{itemize}
    \item Calculate the velocities at $t+\tfrac{1}{2}\Delta t$ using \cref{eq:verlet_halfstep} \hl{(repeated here)}
    \begin{align*}
        \tilde\vvec(t + \tfrac{1}{2}\Delta t) = \vvec(t) + \frac{\Fvec(t)}{m}\frac{\Delta t}{2}.
    \end{align*}
    \item Calculate the new positions at $t + \Delta t$ using \cref{eq:velocity_verlet_positions_halfstep} \hl{(repeated here)}
    \begin{align*}
        \rvec(t + \Delta t) &= \rvec(t) + \tilde\vvec(t+\tfrac{1}{2}\Delta t)\Delta t.
    \end{align*}
    \item Calculate the new forces $\Fvec(t+\Delta t)$/\hl{accelerations $\avec(t+\Delta t)$}.
    \item Calculate the new velocities at $t+\Delta t$ using \cref{eq:verlet_velocity_with_halfstep} \hl{(repeated here)}
    \begin{align*}
        \vvec(t+\Delta t) = \vvec(t + \tfrac{1}{2}\Delta t) + \frac{\Fvec(t + \Delta t)}{m}\frac{\Delta t}{2}.
    \end{align*}
\end{itemize}
This implementation minimizes the memory needs, as we only need to store one copy of $\rvec$, $\vvec$ and $\Fvec$ at all times, compared to implementing \cref{eq:velocity_verlet_position,eq:velocity_verlet_velocity} which needs to store the values of both $\Fvec(t)$ and $\Fvec(t+\Delta)$ to calculate the new velocities \hl{memory usually isn't an issue...}. \todo{maybe also more computationally efficient? flops? floating point truncation?}. 

\section{Boundary conditions}
\todo[inline]{Cite Born and Von Karman 1912? (See Comp. Sim. of Liquids p. 24 (39)}
In theory we now have a working molecular dynamics program by combining \cref{list:simple_md_program,list:calculate_forces,list:calculate_force_between_atoms,list:regular_verlet}. But if we start our simulations we will quicly see that the particles will start spreading out into space, since we haven't implemented any kind of boundary conditions. The particles that are on the surface of our initial system will feel very different forces than the ones in the center\hl{, and, depending on the potential, will not behave as intended?}. When simulating \hl{bulk} liquids and solids we remedy this by using periodic boundary conditions. This means that we repeat the \hl{simulation box} \hl{virtually} an infinite number of times in each direction, so that all atoms \hl{have a bulk-like environment}. Atom $l$ will then have what we call an \hl{\emph{image}/``image''} in all other neighbor boxes, with position
\begin{align}
    \rvec_l^{ijk} = \rvec_l + iL_x + jL_y + kL_z,
    \label{eq:pbc_positions}
\end{align}
where $(L_x, L_y, L_z)$ is the dimensions of the box, and $(i, j, k)$ is the index of the \hl{neighbor box/periodic image box}. In reality we still have the same number of atoms, but the atoms on near the boundaries of the \hl{simulation box} will now be \hl{affected} by the atoms on the opposite side of the \hl{box}.

The first thing we have to do to implement periodic boundary conditinos is to check if any atoms have moved outside the \hl{boundaries} of the system after each time we update the position. If they have moved outside the boundaries of the system we see from \cref{eq:pbc_positions} that we can translate them back into the system by adding or subtracting \hl{system sizes} in each direction. An example of how to do this can be seen in \cref{list:pbc}.
\begin{listing}[!htb]%
% \begin{cppcode*}{gobble=4}
%     void checkBoundaryConditions(System &system, const vec3 &systemSize)
%     {
%         for (Atom *atom : system.atoms())
%         {
%             for (int dim = 0; dim < 3; dim++)
%             {
%                 if (atom->position()[dim] < 0.0) 
%                     atom->position()[dim] += systemSize[dim];
%                 else if (atom->position()[dim] >= systemSize[dim]) 
%                     atom->position()[dim] -= systemSize[dim];
%             }
%         }
%     }
% \end{cppcode*}%
\begin{cppcode*}{gobble=4}
    void checkBoundaryConditions(System &system)
    {
        for (Atom *atom : system.atoms())
        {
            for (int dim = 0; dim < 3; dim++)
            {
                if (atom->position()[dim] < 0.0) 
                    atom->position()[dim] += system.size()[dim];
                else if (atom->position()[dim] >= system.size()[dim]) 
                    atom->position()[dim] -= system.size()[dim];
            }
        }
    }
\end{cppcode*}
\caption{%
    \texttt{checkBoundaryConditions}. \hl{assumes no more than one box outside...}%
    \label{list:pbc}%
}%
\end{listing}%

A consequence of using periodic boundary conditions is that each atom is now \hl{affected} by an infinite number of atoms. To avoid having to do an infinite number of evaluations of the potential we implement something called the \hl{\emph{minimum image convention}/``minimum image convention''} \hl{source/cite?}. This \hl{means/implies} that we only calculate the force between atom $i$ and the \hl{\emph{nearest/closest}} image of each atom $j$, effectively limiting the \hl{reach} of the potential to \hl{half?} the size of the system. \todo{really $L\sqrt{3}$, diagonally} \hl{In doing this we do an approximation, but this is ok??.}

To find the distance between atom $i$ and the closest image of atom $j$ we \hl{just} have to calculate the distance between $i$ and \hl{any image of} $j$, $\rvec_{ij}$, and then check if the length of this vector is more than half the system size in any of the three dimensions. See \cref{list:minimum_image_convention} for an implementation of how to find the distance between a point $u$ to the closest image of a point $v$ using the \emph{minimum image convention}.
\begin{listing}[!htb]%
\begin{cppcode*}{gobble=4}
    double calculateDistanceSquaredUsingMinimumImageConvention(
        const vec3 &u, const vec3 &v, 
        const vec3 &systemSize, const vec3 &halfSystemSize)
    {
        vec3 dr = u - v;
        for (int dim = 0; dim < 3; dim++)
        {
            if (dr[dim] >= halfSystemSize[dim]) dr -= systemSize[dim];
            else if (dr[dim] < -halfSystemSize[dim]) dr += systemSize[dim];
        }
        return dr.lengthSquared(); // Avoid calculating $\sqrt{dr^2}$
    }
\end{cppcode*}
\caption{%
    An example of how to find the distance between two points \texttt{u} and \texttt{v} in a periodic system of size \texttt{systemSize} using the \emph{minimum image convention}. We calculate the distance squared to avoid taking the square root, since this is a slow operation \hl{on a computer}.%
    \label{list:minimum_image_convention}%
}%
\end{listing}%

% The use of periodic boundary conditions has some other consequences for our program. The first thing we need to consider is how to calculate the forces. In theory each atom is now affected by a force from an infinite number of numbers, since we have the image of particle $j$ in an infinte number of repeating periodic boxes. Calculating an infinite number of forces for all atoms in \hl{the periodic box} is unfeasible in a numerical experiment (in nature this infinite sum is incredibly evaluated every timestep $dt = 5.39106(32)\times 10^{-44} \text{s}$ (Planck time)), so we make an approximation where we only consider the force from atoms within the size of the box.

% \hl{force calculation unnecessary} Using the Lennard-Jones potential we see that the force from the potential decays as $1/r^{12}$, meaning that the force from most atoms will be neglible. \cref{fig:lennard-jones_potential}

\chapter{Optimizations?}

\section{Verlet- and cell-lists\label{sec:cell_lists}}
If we try to simulate systems with a lot of atoms using the program we now have developed, we see that the number of evaluations of the potential quickly grow with the number of atoms, scaling as $\mathcal{O}(N^2)$. To optimize the program we can limit the number of evaluations by realizing that the Lennard-Jones in potential decays as $r^{-12}$, meaning that the force between most atoms will be neglible (see also \cref{fig:lennard-jones_potential}). We find that the potential has decreased to $21\%$ of the value at the equilibrium distance, $r_{\text{eq}} =  2^{1/6}\sigma \text{~\AA} \approx 3.8 \text{~\AA}$, at a distance $r_{ij} =  5.5$, and to $0.5\%$ at a distance $r_{ij} =  3\sigma \approx 10.2\text{~\AA}$. \hl{This means that we can skip the force calculations for atoms further apart than} $r_{\text{cutoff}} = 3\sigma$.

The naive implementation of this cutoff length is to do a test inside the force calculation \texttt{calculateForces} in \cref{list:calculate_forces} and see if the distance between atom $i$ and $j$ is greater than the cutoff distance, $r_{ij} > r_\text{cutoff}$, but this still requires the calculation of a lot of interatomic distances. What we do instead is to implement something called \hl{\emph{cell-lists} (different name?)}. This is done by dividing the system into boxes of length 
\begin{align*}
    l &= L/n,
\end{align*}
where
\begin{align*}
    n &= \lfloor  L/r_\text{cutoff} \rfloor,
\end{align*}
which guarantees that $l \geq r_\text{cutoff}$. We then only calculate the force between atom $i$ and all atoms in the box it belongs to, and between it all atoms in the 26 neighboring boxes. Since $l\geq r_\text{cutoff}$, we know that we include all atoms within a radius at least as large as $r_\text{cutoff}$ in the force calculations. See \cref{fig:cell_lists} for an illustration of cell lists in 2 dimensions.
\begin{figure}[htpb]%
    \centering%
    \includesvg[width=0.6\textwidth, svgpath=./images/lennard-jones/]{cell_list02}%
%     \includesvg[width=0.7\textwidth, svgpath=./images/lennard-jones/]{lennard-jones}%
    \caption{%
        An illustration of cell lists in 2 dimensions. We divide the system into cells of size $r \geq r_\text{cutoff}$, and calculate the force between atom $i$ and all atoms in the cell of that atom, and between that atom and all atoms in the 8 neighbor cells (26 neighbor cells in 3 dimensions). %
        \label{fig:cell_lists}%
    }%
\end{figure}%

One issue that arises when using the cell lists is how to utilize Newton's third law. We see that after calculating the force between all atoms in cell $(i,j,k)$ and all atoms in the 26 neighboring cells, adding the calculated forces to the other atoms using Newton's third law, we must not include that cell \hl{(cell ($(i,j,k)$))} in any other force calculations \hl{this timestep}, or else we will get double contributions from atoms in that cell. One simple way of solving this is to keep a list of all cells we have calculated the forces on so far, and then check against that every time we are calculating the force from a neighbor cell. But since we loop through the cells in the same order every time, we see that it would perhaps be more efficent to make a list over neighbor cells for each cell, so that we can just loop through a list of neighbors. \hl{But this is neglible compared to computing forces?}

An implementation of the calculation of forces using cell lists can be seen in \cref{list:cutoff_forcecalculation,list:sort_atoms_into_cells,list:calculateForceFromNeighborCells}. In these examples we don't use Newton's third law for optimization, to make the example simpler. \hl{(we skip Newton's third law to make the example easier to read)}.
%
\begin{listing}[!htb]%
\begin{cppcode*}{gobble=4}
    void calculateForces(System &system)
    {
        ivec3 nCells;
        vector<Atom*> cells = 
            sortAtomsIntoCells(system, cutoffLength, nCells);
        
        // Loop over all cells
        for (int i = 0; i < nCells[0]; i ++)
        for (int j = 0; j < nCells[1]; j ++)
        for (int k = 0; k < nCells[2]; k ++)
        {{{
            for (Atom *atom : cells[i][j][k])
            {
                atom->force() += 
                    calculateForceFromNeighborCells(
                        cells, nCells, atom, i, j, k
                    );
            }
        }}}
    }
\end{cppcode*}
\caption{%
    An example of an implementation of the force calculation \texttt{calculateForces} from \cref{list:simple_md_program}, using the Lennard-Jones potential with a cutoff length for the force, and cell lists. Notice that we don't use Newton's third law, to simplify the example. Examples of how to calculate the forces using cell lists and Newton's third law are described \cref{sec:cell_lists}.%
    \label{list:cutoff_forcecalculation}%
}%
\end{listing}%
%
\begin{listing}[!htb]%
\begin{cppcode*}{gobble=4}
    vector<Atom*> sortAtomsIntoCells(
        System &system, double cutoffLength, ivec3 &nCells)
    {
        nCells = floor(system.size() / cutoffLength);
        ivec3 boxSize = system.size() / vec3(nCells);
        
        vector<Atom*> cells;
        for (Atom *atom : system.atoms())
        {
            ivec3 index = floor(atom->position() / boxSize);
            cells[index[0]][index[1]][index[2]].push_back(atom);
        }
        return cells;
    }
\end{cppcode*}
\caption{%
    An example of an implementation of \texttt{sortAtomsIntoCells} from \cref{list:cutoff_forcecalculation}. This listing shows how to sort atoms into cells for the cell list optimization described in \cref{sec:cell_lists}.%
    \label{list:sort_atoms_into_cells}%
}%
\end{listing}%
%
\begin{listing}[!htb]%
\begin{cppcode*}{gobble=4}
    vec3 calculateForceFromNeighborCells(
        vector<Atom*> &cells, ivec3 &nCells, Atom *atom1, 
        int i1, int j1, int k1)
    {
        vec3 force = zeros<vec3>();
        // Loop over 27 neighbor cells (including self)
        for (int di = -1; di <= 1; di++)
        for (int dj = -1; dj <= 1; dj++)
        for (int dk = -1; dk <= 1; dk++)
        {{{
            // Periodic boundary conditions
            int i2 = (i1 + di + nCells[0]) % nCells[0];
            int j2 = (j1 + dj + nCells[1]) % nCells[1];
            int k2 = (k1 + dk + nCells[2]) % nCells[2];
            // Loop over atoms in neighbor cell
            for (Atom *atom2 : cells[i2][j2][k2])
            {
                if (atom1 == atom2) continue; // Skip i == j
                force() += calculateForceBetweenTwoAtoms(atom1, atom2);
            }
        }}}
        return force;
    }
\end{cppcode*}
\caption{%
    An example of an implementation of \texttt{calculateForceFromNeighborCells} from \cref{list:cutoff_forcecalculation}. This listing shows how to calculate the force on an atom (\texttt{atom1}), from the atoms in the cell it belongs to (\texttt{cells[i1][j1][k1]}), and from the atoms in all 26 neighbor cells.%
    \label{list:calculateForceFromNeighborCells}%
}%
\end{listing}%

\todo[inline]{scaling of cell lists compared to regular, and Verlet lists?}
\todo[inline]{illustration of neighbor box vs. cutoff length?}
\todo[inline]{Update lists every other timestep, $r+dr$, }
